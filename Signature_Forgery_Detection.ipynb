{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LKv5cT_ZJtU7",
    "outputId": "3e6aa306-ab26-42e4-d777-7bd56719814c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import time\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "import keras.backend.tensorflow_backend as tfback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "# tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import fetch_groups\n",
    "from tensorflow_details import _get_available_gpus\n",
    "from model import create_base_network_signet, eucl_dist_output_shape, contrastive_loss, euclidean_distance, load_and_check_model, test_model, predict_score, generate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------#\n",
    "# ----------------------------------------STEP :1-----------------------------------------------------#\n",
    "#--------------------------------Splitting into train & test------------------------------------------#\n",
    "\n",
    "orig_groups, forg_groups = fetch_groups()\n",
    "orig_train, orig_test, forg_train, forg_test = train_test_split(orig_groups, forg_groups, test_size=0.2, random_state=1)\n",
    "orig_train, orig_val, forg_train, forg_val = train_test_split(orig_train, forg_train, test_size=0.25, random_state=1)\n",
    "print(\"Report: Created train, validation and test data sucessfully. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------#\n",
    "# ----------------------------------------STEP :2-----------------------------------------------------#\n",
    "#----------------------------Visualize images to see the signatures-----------------------------------#\n",
    "\n",
    "img_h, img_w = 155, 220\n",
    "\n",
    "def visualize_sample_signature():\n",
    "    '''Function to randomly select a signature from train set and\n",
    "    print two genuine copies and one forged copy'''\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (10, 10))\n",
    "    k = np.random.randint(len(orig_train))\n",
    "    orig_img_names = random.sample(orig_train[k], 2)\n",
    "    forg_img_name = random.sample(forg_train[k], 1)\n",
    "    orig_img1 = cv2.imread(orig_img_names[0], 0)\n",
    "    orig_img2 = cv2.imread(orig_img_names[1], 0)\n",
    "    forg_img = plt.imread(forg_img_name[0], 0)\n",
    "    orig_img1 = cv2.resize(orig_img1, (img_w, img_h))\n",
    "    orig_img2 = cv2.resize(orig_img2, (img_w, img_h))\n",
    "    forg_img = cv2.resize(forg_img, (img_w, img_h))\n",
    "\n",
    "    ax1.imshow(orig_img1, cmap = 'gray')\n",
    "    ax2.imshow(orig_img2, cmap = 'gray')\n",
    "    ax3.imshow(forg_img, cmap = 'gray')\n",
    "\n",
    "    ax1.set_title('Genuine Copy')\n",
    "    ax1.axis('off')\n",
    "    ax2.set_title('Genuine Copy')\n",
    "    ax2.axis('off')\n",
    "    ax3.set_title('Forged Copy')\n",
    "    ax3.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Testing visualize function---------#    \n",
    "visualize_sample_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_signature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------#\n",
    "# -------------------------------------------STEP :3--------------------------------------------------#\n",
    "#-------------------------------------------Run model-------------------------------------------------#\n",
    "\n",
    "tfback._get_available_gpus = _get_available_gpus\n",
    "input_shape=(img_h, img_w, 1)\n",
    "\n",
    "# network definition\n",
    "base_network = create_base_network_signet(input_shape)\n",
    "\n",
    "input_a = Input(shape=(input_shape))\n",
    "input_b = Input(shape=(input_shape))\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "# Compute the Euclidean distance between the two vectors in the latent space\n",
    "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "p_model = Model(input=[input_a, input_b], output=distance, name = 'head_model')\n",
    "\n",
    "num_train_samples = 276*120 + 300*120\n",
    "num_val_samples = num_test_samples = 276*20 + 300*20\n",
    "num_train_samples, num_val_samples, num_test_samples\n",
    "\n",
    "# compile model using RMSProp Optimizer and Contrastive loss function defined above\n",
    "rms = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08)\n",
    "p_model.compile(loss=contrastive_loss, optimizer=rms)\n",
    "p_model.summary()\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=15, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n",
    "    ModelCheckpoint('Weights/model-signet-{epoch:03d}.h5', verbose=1, save_weights_only=True)\n",
    "]\n",
    "\n",
    "batch_sz = 128\n",
    "results = p_model.fit_generator(generate_batch(orig_train, forg_train, batch_sz),\n",
    "                              steps_per_epoch = num_train_samples//batch_sz,\n",
    "                              epochs = 20,\n",
    "                              validation_data = generate_batch(orig_val, forg_val, batch_sz),\n",
    "                              validation_steps = num_val_samples//batch_sz,\n",
    "                              callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------#\n",
    "# -------------------------------------------STEP :4--------------------------------------------------#\n",
    "#------------------------------------Predict Accuracy-------------------------------------------------#\n",
    "\n",
    "acc_thresh = []\n",
    "for i in range(1,20,1):\n",
    "    acc_thresh.append(load_and_check_model('Weights/model-signet-'+str(i).zfill(3)+'.h5'))\n",
    "    print('For model '+str(i)+' Validation Accuracy = ',acc_thresh[i-1][0]*100,'%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, threshold = test_model('Weights/model-signet-020.h5')\n",
    "print(\"Accuracy: {} & Threshold: {}\".format(acc, threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_score()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Signature Forgery Detection",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
